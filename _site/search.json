[
  {
    "objectID": "posts/Welcome/index.html",
    "href": "posts/Welcome/index.html",
    "title": "Welcome to my Data Analytics Platform",
    "section": "",
    "text": "Welcome! This is the introduction to the Advanced Data Analytics Blog.\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post a representing Ordinary Least Squares Linear Regression Model using polynomial regression, local regression, step functions, and both regression and smoothing splines.\nThis was chose due to the level of complexity to showcase a Business Problem of North Carolina’s 2004 birth records data, and the analysis of a correlation between the routines and behaviors of pregnant mothers and childbirth outcomes. Numerous factors were measures such as the father and mother’s age, maturity status of the mother, length of pregnancy, classification of a premature birth, # of visits during the pregnancy, marital status, weight stability of mother during pregnancy, gender of the baby, smoking habit of mother, and race of the mother. This was a total of 13 variables. The target variable in the analysis is the weight outcome of the baby during birth.\nThe study was taken with a total of 1,000 inputs in which all data was gained in North Carolina. Observation with missing data were ommitted from the study- which totalled less than 10%.\nThe data was prepared and split into 70% training data, and 30% testing data. Furthermore, a correlation analysis and visualization were done in order to determine the variable which had the highest absolute value to the target variable, in this case the child’s weight at birth. The variable with the highest correlation was the number of maternity weeks compared to the weight of the infant- which can be seen in the scatter plot demonstrated below. Generally, when maternity weeks were below 27 the weight of the baby was fairly constant at below 3lbs, but entering into the 30th maternity week, the average fetus weight seemed to hit a growth spurt gaining much of its weight up to week 37 in which it plateaus again leading to the constant weight up until birth.\nDuring the analysis the hypothesis was nullified that the mother’s age had any significant correlation to the child’s weight at birth. The father’s weight was compared as well- leading to the same conclusion.\nThe training data was further analyzed in a leaner model in predicting weights compared to weeks of pregnancy as seen below.\nThe overall coefficients are as follows.\n##              Estimate Std. Error t value Pr(&gt;|t|)     ## (Intercept) -7.069515   0.581129 -12.165  &lt; 2e-16 *** ## fage         0.014971   0.009724   1.540  0.12413     ## mage         0.010142   0.009666   1.049  0.29445     ## weeks        0.339776   0.014522  23.397  &lt; 2e-16 *** ## visits       0.012027   0.010808   1.113  0.26622     ## gained       0.008512   0.003048   2.793  0.00537 **\nThis data shows that only weeks of pregnancy and weight gained by the mother had a statistically significant impact unto the child’s birth weight. As shows by a P-value of 2e-16, and .00537, which is of very high significance- especially in the case of weeks of pregnancy. The remaining variables represented a P-value above 0.05 with the mother’s age being the least significant.\nOverall, I would conclude that the data is sound in suitability for testing, with a small amount of limitations. The most significant limitation though, being that the data were all collected within a concentrated area of the country, in that it was only collected in North Carolina. That being considered, other factors could be of significance that were overlooked such as locally economic, environmental, and health accessibility factors that were only pertinent to the North Carolina area. More study would need to be done at a broader region in order to determine if these hypothesis are valid nationwide.\n\nlibrary(knitr)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(rpart)\nlibrary(rsample)\nlibrary(caret)\n\nLoading required package: lattice\n\nAttaching package: 'caret'\n\nThe following object is masked from 'package:purrr':\n\n    lift\n\nlibrary(mgcv)\n\nLoading required package: nlme\n\nAttaching package: 'nlme'\n\nThe following object is masked from 'package:dplyr':\n\n    collapse\n\nThis is mgcv 1.9-0. For overview type 'help(\"mgcv-package\")'.\n\nlibrary(finalfit)\n\nknitr::opts_chunk$set(echo = TRUE)\n\n# Run this code chunk without altering it\n# clear the session\nrm(list=ls())\n\n# Data is stored in a csv file, the first row contains the variable names. \n# we call our data mydata\nmydata&lt;-read.csv (\"Data_RLab5.csv\", header=TRUE)\n\n# remove lowbirthweight\nmydata&lt;-mydata%&gt;%\n  select(-lowbirthweight)\n\n# Please provide your code for Task 1 in this code chunk\n\n# Please provide your code for Task 1 in this code chunk\nlibrary(ISLR)\nlibrary(lubridate)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(knitr)\nlibrary(dygraphs)\nlibrary(xts)\n\nLoading required package: zoo\n\nAttaching package: 'zoo'\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n\n\n######################### Warning from 'xts' package ##########################\n#                                                                             #\n# The dplyr lag() function breaks how base R's lag() function is supposed to  #\n# work, which breaks lag(my_xts). Calls to lag(my_xts) that you type or       #\n# source() into this session won't work correctly.                            #\n#                                                                             #\n# Use stats::lag() to make sure you're not using dplyr::lag(), or you can add #\n# conflictRules('dplyr', exclude = 'lag') to your .Rprofile to stop           #\n# dplyr from breaking base R's lag() function.                                #\n#                                                                             #\n# Code in packages is not affected. It's protected by R's namespace mechanism #\n# Set `options(xts.warn_dplyr_breaks_lag = FALSE)` to suppress this warning.  #\n#                                                                             #\n###############################################################################\n\nAttaching package: 'xts'\n\nThe following objects are masked from 'package:dplyr':\n\n    first, last\n\nlibrary(purrr)\nlibrary(SmartEDA)\n\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2\n\n# Verifying data sctructure and any missing values\nExpData(data=mydata,type=1)\n\n                                          Descriptions   Value\n1                                   Sample size (nrow)     999\n2                              No. of variables (ncol)      12\n3                    No. of numeric/interger variables       6\n4                              No. of factor variables       0\n5                                No. of text variables       6\n6                             No. of logical variables       0\n7                          No. of identifier variables       0\n8                                No. of date variables       0\n9             No. of zero variance variables (uniform)       0\n10               %. of variables having complete cases 50% (6)\n11   %. of variables having &gt;0% and &lt;50% missing cases 50% (6)\n12 %. of variables having &gt;=50% and &lt;90% missing cases  0% (0)\n13          %. of variables having &gt;=90% missing cases  0% (0)\n\nExpData(data=mydata,type=2)\n\n   Index Variable_Name Variable_Type Sample_n Missing_Count Per_of_Missing\n1      1          fage       integer      829           170          0.170\n2      2          mage       integer      999             0          0.000\n3      3        mature     character      999             0          0.000\n4      4         weeks       integer      998             1          0.001\n5      5        premie     character      998             1          0.001\n6      6        visits       integer      991             8          0.008\n7      7       marital     character      999             0          0.000\n8      8        gained       integer      973            26          0.026\n9      9        weight       numeric      999             0          0.000\n10    10        gender     character      999             0          0.000\n11    11         habit     character      999             0          0.000\n12    12      whitemom     character      997             2          0.002\n   No_of_distinct_values\n1                     37\n2                     33\n3                      2\n4                     23\n5                      3\n6                     26\n7                      2\n8                     71\n9                    126\n10                     2\n11                     2\n12                     3\n\ndf&lt;- data.frame(mydata)\nff_glimpse(mydata)\n\n$Continuous\n        label var_type   n missing_n missing_percent mean   sd  min quartile_25\nfage     fage    &lt;int&gt; 829       170            17.0 30.3  6.8 14.0        25.0\nmage     mage    &lt;int&gt; 999         0             0.0 27.0  6.2 13.0        22.0\nweeks   weeks    &lt;int&gt; 998         1             0.1 38.3  2.9 20.0        37.0\nvisits visits    &lt;int&gt; 991         8             0.8 12.1  4.0  0.0        10.0\ngained gained    &lt;int&gt; 973        26             2.6 30.3 14.2  0.0        20.0\nweight weight    &lt;dbl&gt; 999         0             0.0  7.1  1.5  1.0         6.4\n       median quartile_75  max\nfage     30.0        35.0 55.0\nmage     27.0        32.0 50.0\nweeks    39.0        40.0 45.0\nvisits   12.0        15.0 30.0\ngained   30.0        38.0 85.0\nweight    7.3         8.1 11.8\n\n$Categorical\n            label var_type   n missing_n missing_percent levels_n levels\nmature     mature    &lt;chr&gt; 999         0             0.0        2      -\npremie     premie    &lt;chr&gt; 998         1             0.1        2      -\nmarital   marital    &lt;chr&gt; 999         0             0.0        2      -\ngender     gender    &lt;chr&gt; 999         0             0.0        2      -\nhabit       habit    &lt;chr&gt; 999         0             0.0        2      -\nwhitemom whitemom    &lt;chr&gt; 997         2             0.2        2      -\n         levels_count levels_percent\nmature              -              -\npremie              -              -\nmarital             -              -\ngender              -              -\nhabit               -              -\nwhitemom            -              -\n\nstr(mydata)\n\n'data.frame':   999 obs. of  12 variables:\n $ fage    : int  NA NA 19 21 NA NA 18 17 NA 20 ...\n $ mage    : int  13 14 15 15 15 15 15 15 16 16 ...\n $ mature  : chr  \"younger mom\" \"younger mom\" \"younger mom\" \"younger mom\" ...\n $ weeks   : int  39 42 37 41 39 38 37 35 38 37 ...\n $ premie  : chr  \"full term\" \"full term\" \"full term\" \"full term\" ...\n $ visits  : int  10 15 11 6 9 19 12 5 9 13 ...\n $ marital : chr  \"married\" \"married\" \"married\" \"married\" ...\n $ gained  : int  38 20 38 34 27 22 76 15 NA 52 ...\n $ weight  : num  7.63 7.88 6.63 8 6.38 5.38 8.44 4.69 8.81 6.94 ...\n $ gender  : chr  \"male\" \"male\" \"female\" \"male\" ...\n $ habit   : chr  \"nonsmoker\" \"nonsmoker\" \"nonsmoker\" \"nonsmoker\" ...\n $ whitemom: chr  \"not white\" \"not white\" \"white\" \"white\" ...\n\nknitr::kable(head(mydata, n=1))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfage\nmage\nmature\nweeks\npremie\nvisits\nmarital\ngained\nweight\ngender\nhabit\nwhitemom\n\n\n\n\nNA\n13\nyounger mom\n39\nfull term\n10\nmarried\n38\n7.63\nmale\nnonsmoker\nnot white\n\n\n\n\n#checking missing values\nsapply(mydata, function(x) sum(is.na(x)))\n\n    fage     mage   mature    weeks   premie   visits  marital   gained \n     170        0        0        1        1        8        0       26 \n  weight   gender    habit whitemom \n       0        0        0        2 \n\n#replace numerical with median\nglimpse(df)\n\nRows: 999\nColumns: 12\n$ fage     &lt;int&gt; NA, NA, 19, 21, NA, NA, 18, 17, NA, 20, 30, NA, NA, NA, 21, N…\n$ mage     &lt;int&gt; 13, 14, 15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 1…\n$ mature   &lt;chr&gt; \"younger mom\", \"younger mom\", \"younger mom\", \"younger mom\", \"…\n$ weeks    &lt;int&gt; 39, 42, 37, 41, 39, 38, 37, 35, 38, 37, 45, 42, 40, 38, 38, 4…\n$ premie   &lt;chr&gt; \"full term\", \"full term\", \"full term\", \"full term\", \"full ter…\n$ visits   &lt;int&gt; 10, 15, 11, 6, 9, 19, 12, 5, 9, 13, 9, 8, 4, 12, 15, 7, 12, 5…\n$ marital  &lt;chr&gt; \"married\", \"married\", \"married\", \"married\", \"married\", \"marri…\n$ gained   &lt;int&gt; 38, 20, 38, 34, 27, 22, 76, 15, NA, 52, 28, 34, 12, 30, 75, 3…\n$ weight   &lt;dbl&gt; 7.63, 7.88, 6.63, 8.00, 6.38, 5.38, 8.44, 4.69, 8.81, 6.94, 7…\n$ gender   &lt;chr&gt; \"male\", \"male\", \"female\", \"male\", \"female\", \"male\", \"male\", \"…\n$ habit    &lt;chr&gt; \"nonsmoker\", \"nonsmoker\", \"nonsmoker\", \"nonsmoker\", \"nonsmoke…\n$ whitemom &lt;chr&gt; \"not white\", \"not white\", \"white\", \"white\", \"not white\", \"not…\n\ndf.noNAs &lt;- df %&gt;% mutate(across(where(is.numeric), ~replace_na(., median(., na.rm=TRUE))))\nglimpse(df.noNAs)\n\nRows: 999\nColumns: 12\n$ fage     &lt;int&gt; 30, 30, 19, 21, 30, 30, 18, 17, 30, 20, 30, 30, 30, 30, 21, 3…\n$ mage     &lt;int&gt; 13, 14, 15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 1…\n$ mature   &lt;chr&gt; \"younger mom\", \"younger mom\", \"younger mom\", \"younger mom\", \"…\n$ weeks    &lt;int&gt; 39, 42, 37, 41, 39, 38, 37, 35, 38, 37, 45, 42, 40, 38, 38, 4…\n$ premie   &lt;chr&gt; \"full term\", \"full term\", \"full term\", \"full term\", \"full ter…\n$ visits   &lt;int&gt; 10, 15, 11, 6, 9, 19, 12, 5, 9, 13, 9, 8, 4, 12, 15, 7, 12, 5…\n$ marital  &lt;chr&gt; \"married\", \"married\", \"married\", \"married\", \"married\", \"marri…\n$ gained   &lt;int&gt; 38, 20, 38, 34, 27, 22, 76, 15, 30, 52, 28, 34, 12, 30, 75, 3…\n$ weight   &lt;dbl&gt; 7.63, 7.88, 6.63, 8.00, 6.38, 5.38, 8.44, 4.69, 8.81, 6.94, 7…\n$ gender   &lt;chr&gt; \"male\", \"male\", \"female\", \"male\", \"female\", \"male\", \"male\", \"…\n$ habit    &lt;chr&gt; \"nonsmoker\", \"nonsmoker\", \"nonsmoker\", \"nonsmoker\", \"nonsmoke…\n$ whitemom &lt;chr&gt; \"not white\", \"not white\", \"white\", \"white\", \"not white\", \"not…\n\n# replace categorial with mode\nMode &lt;- function(x) {\n    ux &lt;- unique(x)\n    ux[which.max(tabulate(match(x, ux)))]\n}\n\ndf.noNAs[is.na(df.noNAs)] &lt;- Mode(df.noNAs)\nExpData(data=df.noNAs,type=2)\n\n   Index Variable_Name Variable_Type Sample_n Missing_Count Per_of_Missing\n1      1          fage       integer      999             0              0\n2      2          mage       integer      999             0              0\n3      3        mature     character      999             0              0\n4      4         weeks       integer      999             0              0\n5      5        premie          list      999             0              0\n6      6        visits       integer      999             0              0\n7      7       marital     character      999             0              0\n8      8        gained       integer      999             0              0\n9      9        weight       numeric      999             0              0\n10    10        gender     character      999             0              0\n11    11         habit     character      999             0              0\n12    12      whitemom          list      999             0              0\n   No_of_distinct_values\n1                     37\n2                     33\n3                      2\n4                     23\n5                      3\n6                     26\n7                      2\n8                     71\n9                    126\n10                     2\n11                     2\n12                     3\n\n#identify and removal of categorial variables\n\nstr(df.noNAs)\n\n'data.frame':   999 obs. of  12 variables:\n $ fage    : int  30 30 19 21 30 30 18 17 30 20 ...\n $ mage    : int  13 14 15 15 15 15 15 15 16 16 ...\n $ mature  : chr  \"younger mom\" \"younger mom\" \"younger mom\" \"younger mom\" ...\n $ weeks   : int  39 42 37 41 39 38 37 35 38 37 ...\n $ premie  :List of 999\n  ..$ : chr \"full term\"\n  ..$ : chr \"full term\"\n  ..$ : chr \"full term\"\n  ..$ : chr \"full term\"\n  ..$ : chr \"full term\"\n  ..$ : chr \"full term\"\n  ..$ : chr \"full term\"\n  ..$ : chr \"premie\"\n  ..$ : chr \"full term\"\n  ..$ : chr \"full term\"\n  ..$ : chr \"full term\"\n  ..$ : chr \"full term\"\n  ..$ : chr \"full term\"\n  ..$ : chr \"full term\"\n  ..$ : chr \"full term\"\n  ..$ : chr \"full term\"\n  ..$ : chr \"full term\"\n  ..$ : chr \"premie\"\n  ..$ : chr \"full term\"\n  ..$ : chr \"full term\"\n  ..$ : chr \"full term\"\n  ..$ : chr \"full term\"\n  ..$ : chr \"full term\"\n  ..$ : chr \"full term\"\n  ..$ : chr \"full term\"\n  ..$ : chr \"full term\"\n  ..$ : chr \"full term\"\n  ..$ : chr \"full term\"\n  ..$ : chr \"full term\"\n  ..$ : chr \"full term\"\n  ..$ : chr \"full term\"\n  ..$ : chr \"premie\"\n  ..$ : chr \"premie\"\n  ..$ : chr \"premie\"\n  ..$ : chr \"premie\"\n  ..$ : chr \"premie\"\n  ..$ : chr \"premie\"\n  ..$ : chr \"full term\"\n  ..$ : chr \"full term\"\n  ..$ : chr \"full term\"\n  ..$ : chr \"full term\"\n  ..$ : chr \"full term\"\n  ..$ : chr \"full term\"\n  ..$ : chr \"full term\"\n  ..$ : chr \"full term\"\n  ..$ : chr \"full term\"\n  ..$ : chr \"full term\"\n  ..$ : chr \"full term\"\n  ..$ : chr \"full term\"\n  ..$ : chr \"full term\"\n  ..$ : chr \"full term\"\n  ..$ : chr \"full term\"\n  ..$ : chr \"full term\"\n  ..$ : chr \"full term\"\n  ..$ : chr \"full term\"\n  ..$ : chr \"full term\"\n  ..$ : chr \"full term\"\n  ..$ : chr \"full term\"\n  ..$ : chr \"full term\"\n  ..$ : chr \"full term\"\n  ..$ : chr \"full term\"\n  ..$ : chr \"full term\"\n  ..$ : chr \"full term\"\n  ..$ : chr \"full term\"\n  ..$ : chr \"full term\"\n  ..$ : chr \"full term\"\n  ..$ : chr \"full term\"\n  ..$ : chr \"full term\"\n  ..$ : chr \"full term\"\n  ..$ : chr \"full term\"\n  ..$ : chr \"premie\"\n  ..$ : chr \"premie\"\n  ..$ : chr \"premie\"\n  ..$ : chr \"premie\"\n  ..$ : chr \"premie\"\n  ..$ : chr \"full term\"\n  ..$ : chr \"full term\"\n  ..$ : chr \"full term\"\n  ..$ : chr \"full term\"\n  ..$ : chr \"full term\"\n  ..$ : chr \"full term\"\n  ..$ : chr \"full term\"\n  ..$ : chr \"full term\"\n  ..$ : chr \"full term\"\n  ..$ : chr \"full term\"\n  ..$ : chr \"full term\"\n  ..$ : chr \"full term\"\n  ..$ : chr \"full term\"\n  ..$ : chr \"full term\"\n  ..$ : chr \"full term\"\n  ..$ : chr \"full term\"\n  ..$ : chr \"full term\"\n  ..$ : chr \"full term\"\n  ..$ : chr \"full term\"\n  ..$ : chr \"full term\"\n  ..$ : chr \"full term\"\n  ..$ : chr \"full term\"\n  ..$ : chr \"full term\"\n  ..$ : chr \"full term\"\n  .. [list output truncated]\n $ visits  : int  10 15 11 6 9 19 12 5 9 13 ...\n $ marital : chr  \"married\" \"married\" \"married\" \"married\" ...\n $ gained  : int  38 20 38 34 27 22 76 15 30 52 ...\n $ weight  : num  7.63 7.88 6.63 8 6.38 5.38 8.44 4.69 8.81 6.94 ...\n $ gender  : chr  \"male\" \"male\" \"female\" \"male\" ...\n $ habit   : chr  \"nonsmoker\" \"nonsmoker\" \"nonsmoker\" \"nonsmoker\" ...\n $ whitemom:List of 999\n  ..$ : chr \"not white\"\n  ..$ : chr \"not white\"\n  ..$ : chr \"white\"\n  ..$ : chr \"white\"\n  ..$ : chr \"not white\"\n  ..$ : chr \"not white\"\n  ..$ : chr \"not white\"\n  ..$ : chr \"not white\"\n  ..$ : chr \"white\"\n  ..$ : chr \"white\"\n  ..$ : chr \"white\"\n  ..$ : chr \"white\"\n  ..$ : chr \"not white\"\n  ..$ : chr \"white\"\n  ..$ : chr \"white\"\n  ..$ : chr \"white\"\n  ..$ : chr \"white\"\n  ..$ : chr \"not white\"\n  ..$ : chr \"white\"\n  ..$ : chr \"not white\"\n  ..$ : chr \"not white\"\n  ..$ : chr \"white\"\n  ..$ : chr \"not white\"\n  ..$ : chr \"not white\"\n  ..$ : chr \"white\"\n  ..$ : chr \"not white\"\n  ..$ : chr \"not white\"\n  ..$ : chr \"not white\"\n  ..$ : chr \"white\"\n  ..$ : chr \"not white\"\n  ..$ : chr \"white\"\n  ..$ : chr \"not white\"\n  ..$ : chr \"not white\"\n  ..$ : chr \"not white\"\n  ..$ : chr \"white\"\n  ..$ : chr \"not white\"\n  ..$ : chr \"white\"\n  ..$ : chr \"white\"\n  ..$ : chr \"not white\"\n  ..$ : chr \"not white\"\n  ..$ : chr \"not white\"\n  ..$ : chr \"not white\"\n  ..$ : chr \"white\"\n  ..$ : chr \"white\"\n  ..$ : chr \"white\"\n  ..$ : chr \"white\"\n  ..$ : chr \"not white\"\n  ..$ : chr \"white\"\n  ..$ : chr \"white\"\n  ..$ : chr \"white\"\n  ..$ : chr \"not white\"\n  ..$ : chr \"white\"\n  ..$ : chr \"white\"\n  ..$ : chr \"not white\"\n  ..$ : chr \"white\"\n  ..$ : chr \"not white\"\n  ..$ : chr \"not white\"\n  ..$ : chr \"not white\"\n  ..$ : chr \"not white\"\n  ..$ : chr \"not white\"\n  ..$ : chr \"not white\"\n  ..$ : chr \"not white\"\n  ..$ : chr \"not white\"\n  ..$ : chr \"white\"\n  ..$ : chr \"white\"\n  ..$ : chr \"white\"\n  ..$ : chr \"white\"\n  ..$ : chr \"white\"\n  ..$ : chr \"white\"\n  ..$ : chr \"not white\"\n  ..$ : chr \"white\"\n  ..$ : chr \"white\"\n  ..$ : chr \"white\"\n  ..$ : chr \"white\"\n  ..$ : chr \"white\"\n  ..$ : chr \"white\"\n  ..$ : chr \"white\"\n  ..$ : chr \"white\"\n  ..$ : chr \"not white\"\n  ..$ : chr \"not white\"\n  ..$ : chr \"white\"\n  ..$ : chr \"white\"\n  ..$ : chr \"not white\"\n  ..$ : chr \"not white\"\n  ..$ : chr \"white\"\n  ..$ : chr \"not white\"\n  ..$ : chr \"white\"\n  ..$ : chr \"white\"\n  ..$ : chr \"not white\"\n  ..$ : chr \"white\"\n  ..$ : chr \"white\"\n  ..$ : chr \"not white\"\n  ..$ : chr \"not white\"\n  ..$ : chr \"white\"\n  ..$ : chr \"white\"\n  ..$ : chr \"not white\"\n  ..$ : chr \"not white\"\n  ..$ : chr \"white\"\n  ..$ : chr \"not white\"\n  .. [list output truncated]\n\nno_cat&lt;- df.noNAs %&gt;% select(-mature, - premie, - marital, - gender, - habit, - whitemom)\nhead(no_cat)\n\n  fage mage weeks visits gained weight\n1   30   13    39     10     38   7.63\n2   30   14    42     15     20   7.88\n3   19   15    37     11     38   6.63\n4   21   15    41      6     34   8.00\n5   30   15    39      9     27   6.38\n6   30   15    38     19     22   5.38\n\n round(cor(no_cat), digits = 2)\n\n        fage  mage weeks visits gained weight\nfage    1.00  0.71 -0.01   0.07  -0.04   0.06\nmage    0.71  1.00 -0.03   0.16  -0.06   0.06\nweeks  -0.01 -0.03  1.00   0.17   0.09   0.67\nvisits  0.07  0.16  0.17   1.00   0.06   0.13\ngained -0.04 -0.06  0.09   0.06   1.00   0.15\nweight  0.06  0.06  0.67   0.13   0.15   1.00\n\n # Weeks has the highest correlation in comparison with weight (target variable) at 0.67\n \n #Scatterplot of relationship between weeks and weight \n ggplot(data= no_cat, mapping = aes(x = weeks, y = weight))+geom_point()\n\n\n\n #generally speaking when weeks are below 27 the weight is fairly consistent in that weight is below 3- with the exception of one outlier.  Then above 27 weeks the correlation is seen very well in that the more weeks accrued the more weight gained until about 37 weeks when the range increases significantly.  The weight at 37 weeks - 40 weeks varies from 3-12 until about 43rd week where the variation is reduced to the weight range of 6-9.  \n \n# Please provide your code for Task 2 in this code chunk\n# split the sample by using rsample package\n\n# Split the data into a training set (70%) and a test set (30%)\nset.seed(123456)\nsplit&lt;- initial_split(no_cat, prop = 0.70, strata = \"weight\")\ntrain_data &lt;-training(split)\ntest_data&lt;-testing(split)\ndim(train_data)\n\n[1] 698   6\n\ndim(test_data)\n\n[1] 301   6\n\n# Please provide your code for Task 3  in this code chunk\nlibrary(readxl)\nlibrary(GGally)\nlibrary(psych)\n\n\nAttaching package: 'psych'\n\nThe following objects are masked from 'package:ggplot2':\n\n    %+%, alpha\n\nlibrary(mgcv)\n#lm function to estimate\nlinearmodel&lt;- lm(weight ~ fage+mage+weeks+visits+gained, data = train_data)\nsummary(linearmodel)\n\n\nCall:\nlm(formula = weight ~ fage + mage + weeks + visits + gained, \n    data = train_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.5095 -0.7245 -0.0344  0.7819  4.3711 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -7.069515   0.581129 -12.165  &lt; 2e-16 ***\nfage         0.014971   0.009724   1.540  0.12413    \nmage         0.010142   0.009666   1.049  0.29445    \nweeks        0.339776   0.014522  23.397  &lt; 2e-16 ***\nvisits       0.012027   0.010808   1.113  0.26622    \ngained       0.008512   0.003048   2.793  0.00537 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.102 on 692 degrees of freedom\nMultiple R-squared:  0.4722,    Adjusted R-squared:  0.4683 \nF-statistic: 123.8 on 5 and 692 DF,  p-value: &lt; 2.2e-16\n\n#predict function on test_data\npredicted_weights_ols&lt;- predict(linearmodel, test_data)\npredicted_weights_ols\n\n       1        2        3        4        5        6        7        8 \n7.206459 8.142845 7.689404 6.396338 7.521283 6.791298 6.863208 7.259949 \n       9       10       11       12       13       14       15       16 \n7.604745 6.969011 7.752944 7.057828 6.941123 6.332564 7.767024 6.929965 \n      17       18       19       20       21       22       23       24 \n8.837429 6.353081 7.449417 6.799806 6.972673 7.589810 7.746632 7.848207 \n      25       26       27       28       29       30       31       32 \n5.294945 5.955056 6.092983 7.129457 7.402918 6.292921 6.746848 7.793767 \n      33       34       35       36       37       38       39       40 \n8.256526 6.905086 7.179371 6.433591 7.917666 8.278453 6.828455 7.021316 \n      41       42       43       44       45       46       47       48 \n7.202876 7.940847 7.408036 7.824577 7.609976 6.495756 4.826169 5.177585 \n      49       50       51       52       53       54       55       56 \n5.056527 6.207052 6.928716 6.939441 6.958956 7.582710 7.930428 8.064591 \n      57       58       59       60       61       62       63       64 \n7.050143 7.327507 7.984173 6.564225 7.440815 5.502110 6.780215 7.041163 \n      65       66       67       68       69       70       71       72 \n7.327632 7.501869 6.574142 7.224345 6.590618 7.518297 7.294746 7.902848 \n      73       74       75       76       77       78       79       80 \n7.120466 7.126307 7.539432 7.333818 4.144065 6.818768 8.187750 7.904615 \n      81       82       83       84       85       86       87       88 \n6.891745 7.081912 7.309021 6.695119 8.450645 6.966621 6.480971 7.371525 \n      89       90       91       92       93       94       95       96 \n6.541723 6.384809 6.211872 5.006255 7.593739 7.089950 7.289222 6.647254 \n      97       98       99      100      101      102      103      104 \n8.233174 7.934431 7.012843 9.030049 7.247574 7.232010 7.436129 6.863732 \n     105      106      107      108      109      110      111      112 \n7.196228 7.758461 6.131373 5.661927 6.601054 6.906781 7.728074 7.773756 \n     113      114      115      116      117      118      119      120 \n6.816780 7.072575 6.647445 7.621225 7.223053 7.264403 7.151267 7.473644 \n     121      122      123      124      125      126      127      128 \n6.908264 7.532909 7.297082 8.311321 8.034693 7.236584 6.808971 7.377754 \n     129      130      131      132      133      134      135      136 \n7.515271 6.818418 7.497400 7.802950 7.516205 5.903862 5.695574 6.950652 \n     137      138      139      140      141      142      143      144 \n7.009943 7.304530 8.392232 7.031666 7.977330 7.687166 8.213341 7.574134 \n     145      146      147      148      149      150      151      152 \n7.658685 7.813840 7.987347 7.305348 8.482849 8.340861 8.974637 8.349672 \n     153      154      155      156      157      158      159      160 \n7.601271 4.457111 5.011991 6.846349 7.912964 6.562691 8.552852 7.619833 \n     161      162      163      164      165      166      167      168 \n7.518851 7.491356 7.103079 7.049750 7.352806 7.247764 7.272388 5.782700 \n     169      170      171      172      173      174      175      176 \n7.410985 7.788144 7.295767 7.527060 9.062128 6.578398 6.969795 8.014959 \n     177      178      179      180      181      182      183      184 \n7.068404 7.200625 6.960985 2.575002 5.981847 7.358050 8.028295 7.170602 \n     185      186      187      188      189      190      191      192 \n6.942352 6.913695 7.325580 6.953786 9.187355 6.520969 6.432035 7.222495 \n     193      194      195      196      197      198      199      200 \n7.413329 7.144888 5.901870 1.510635 5.516633 7.337161 7.338666 8.520274 \n     201      202      203      204      205      206      207      208 \n6.836956 7.905900 7.395190 6.936931 7.021949 8.168934 7.124095 7.887856 \n     209      210      211      212      213      214      215      216 \n6.968182 6.801321 7.036622 6.707389 7.128750 6.686850 7.852325 7.930119 \n     217      218      219      220      221      222      223      224 \n8.049838 7.686626 6.359693 6.364713 7.321387 7.591973 7.958059 7.013013 \n     225      226      227      228      229      230      231      232 \n8.654337 6.706395 7.710186 7.554958 6.894553 7.161805 7.493389 7.284667 \n     233      234      235      236      237      238      239      240 \n5.242222 3.180773 5.547769 8.623435 7.199718 7.006131 6.965326 6.980275 \n     241      242      243      244      245      246      247      248 \n6.499306 6.796639 7.371286 7.431327 7.169776 7.169301 8.147073 6.403166 \n     249      250      251      252      253      254      255      256 \n6.311015 8.176452 7.219770 7.476066 7.167192 7.928256 7.462280 6.767757 \n     257      258      259      260      261      262      263      264 \n7.021819 8.163833 6.706689 7.485461 7.235941 7.906234 8.131289 6.347607 \n     265      266      267      268      269      270      271      272 \n7.741456 7.350746 7.836845 7.712357 7.472422 6.857357 7.195949 5.911672 \n     273      274      275      276      277      278      279      280 \n6.551110 6.575462 3.642185 7.391555 7.003672 7.432312 7.430921 6.982839 \n     281      282      283      284      285      286      287      288 \n4.701394 7.571214 7.530296 7.234493 7.417701 7.841886 6.957809 7.598812 \n     289      290      291      292      293      294      295      296 \n8.184160 8.353811 0.816149 4.894311 7.875087 7.426209 7.812333 8.255838 \n     297      298      299      300      301 \n6.299358 6.340733 7.121022 7.494825 5.062497 \n\n#calculating MSPE\npredict(linearmodel, newdata = test_data, interval = \"prediction\")\n\n         fit        lwr       upr\n1   7.206459  5.0258781  9.387041\n2   8.142845  5.9584115 10.327279\n3   7.689404  5.5142955  9.864512\n4   6.396338  4.2242397  8.568436\n5   7.521283  5.3474988  9.695067\n6   6.791298  4.6216358  8.960960\n7   6.863208  4.6900601  9.036355\n8   7.259949  5.0861020  9.433796\n9   7.604745  5.4312864  9.778203\n10  6.969011  4.7937471  9.144275\n11  7.752944  5.5822309  9.923657\n12  7.057828  4.8877716  9.227884\n13  6.941123  4.7553423  9.126905\n14  6.332564  4.1592026  8.505926\n15  7.767024  5.5955203  9.938528\n16  6.929965  4.7561969  9.103733\n17  8.837429  6.6564419 11.018417\n18  6.353081  4.1835424  8.522620\n19  7.449417  5.2783748  9.620458\n20  6.799806  4.6302349  8.969376\n21  6.972673  4.7969529  9.148392\n22  7.589810  5.4112432  9.768377\n23  7.746632  5.5744108  9.918853\n24  7.848207  5.6777648 10.018650\n25  5.294945  3.1106278  7.479262\n26  5.955056  3.7783149  8.131796\n27  6.092983  3.9237749  8.262192\n28  7.129457  4.9547666  9.304148\n29  7.402918  5.2326717  9.573164\n30  6.292921  4.1228515  8.462991\n31  6.746848  4.5778744  8.915822\n32  7.793767  5.6233171  9.964216\n33  8.256526  6.0860765 10.426975\n34  6.905086  4.7332630  9.076910\n35  7.179371  5.0061719  9.352571\n36  6.433591  4.2565795  8.610602\n37  7.917666  5.7382544 10.097078\n38  8.278453  6.1013408 10.455564\n39  6.828455  4.6602084  8.996702\n40  7.021316  4.8509787  9.191654\n41  7.202876  5.0331451  9.372607\n42  7.940847  5.7701803 10.111514\n43  7.408036  5.2338721  9.582201\n44  7.824577  5.6530981  9.996056\n45  7.609976  5.4282009  9.791752\n46  6.495756  4.3263255  8.665186\n47  4.826169  2.6396642  7.012673\n48  5.177585  2.9996014  7.355569\n49  5.056527  2.8814034  7.231650\n50  6.207052  4.0371982  8.376907\n51  6.928716  4.7587250  9.098708\n52  6.939441  4.7647099  9.114172\n53  6.958956  4.7914934  9.126419\n54  7.582710  5.4136429  9.751778\n55  7.930428  5.7616868 10.099170\n56  8.064591  5.8896012 10.239581\n57  7.050143  4.8806747  9.219611\n58  7.327507  5.1551805  9.499834\n59  7.984173  5.8144183 10.153928\n60  6.564225  4.3846766  8.743773\n61  7.440815  5.2680518  9.613578\n62  5.502110  3.3303821  7.673839\n63  6.780215  4.6064367  8.953993\n64  7.041163  4.8707687  9.211557\n65  7.327632  5.1583311  9.496933\n66  7.501869  5.3306172  9.673120\n67  6.574142  4.4067684  8.741515\n68  7.224345  5.0580340  9.390657\n69  6.590618  4.4210828  8.760153\n70  7.518297  5.3489578  9.687635\n71  7.294746  5.1226458  9.466847\n72  7.902848  5.7091204 10.096576\n73  7.120466  4.9511249  9.289807\n74  7.126307  4.9593730  9.293240\n75  7.539432  5.3681615  9.710702\n76  7.333818  5.1662667  9.501370\n77  4.144065  1.9637592  6.324370\n78  6.818768  4.6279117  9.009624\n79  8.187750  6.0182876 10.357213\n80  7.904615  5.7362541 10.072976\n81  6.891745  4.7246789  9.058811\n82  7.081912  4.9080008  9.255824\n83  7.309021  5.1425301  9.475512\n84  6.695119  4.5258622  8.864376\n85  8.450645  6.2744425 10.626848\n86  6.966621  4.8002072  9.133035\n87  6.480971  4.3131031  8.648840\n88  7.371525  5.1950766  9.547973\n89  6.541723  4.3748065  8.708640\n90  6.384809  4.2147831  8.554835\n91  6.211872  4.0392108  8.384534\n92  5.006255  2.8287984  7.183711\n93  7.593739  5.4202353  9.767242\n94  7.089950  4.9222902  9.257609\n95  7.289222  5.1227382  9.455705\n96  6.647254  4.4788939  8.815615\n97  8.233174  6.0620288 10.404319\n98  7.934431  5.7641533 10.104709\n99  7.012843  4.8457553  9.179931\n100 9.030049  6.8557504 11.204348\n101 7.247574  5.0814662  9.413681\n102 7.232010  5.0664356  9.397585\n103 7.436129  5.2379015  9.634356\n104 6.863732  4.6962991  9.031164\n105 7.196228  5.0275505  9.364906\n106 7.758461  5.5904088  9.926514\n107 6.131373  3.9607754  8.301970\n108 5.661927  3.4857415  7.838112\n109 6.601054  4.4353785  8.766729\n110 6.906781  4.7412731  9.072290\n111 7.728074  5.5610695  9.895079\n112 7.773756  5.6031854  9.944327\n113 6.816780  4.6470143  8.986545\n114 7.072575  4.9039168  9.241233\n115 6.647445  4.4728541  8.822037\n116 7.621225  5.4526458  9.789805\n117 7.223053  5.0560697  9.390036\n118 7.264403  5.0942743  9.434531\n119 7.151267  4.9401032  9.362431\n120 7.473644  5.3046082  9.642681\n121 6.908264  4.7421749  9.074354\n122 7.532909  5.3664919  9.699327\n123 7.297082  5.1273668  9.466797\n124 8.311321  6.1388064 10.483836\n125 8.034693  5.8670201 10.202365\n126 7.236584  5.0708682  9.402301\n127 6.808971  4.6347885  8.983153\n128 7.377754  5.2115673  9.543941\n129 7.515271  5.3455165  9.685025\n130 6.818418  4.6517027  8.985132\n131 7.497400  5.3301232  9.664676\n132 7.802950  5.6339060  9.971995\n133 7.516205  5.3480656  9.684345\n134 5.903862  3.7321029  8.075622\n135 5.695574  3.5150706  7.876077\n136 6.950652  4.7845968  9.116707\n137 7.009943  4.8424602  9.177425\n138 7.304530  5.1368674  9.472193\n139 8.392232  6.2240484 10.560416\n140 7.031666  4.8656273  9.197705\n141 7.977330  5.8107971 10.143863\n142 7.687166  5.5211622  9.853169\n143 8.213341  6.0339830 10.392698\n144 7.574134  5.4068078  9.741460\n145 7.658685  5.4927163  9.824654\n146 7.813840  5.6445262  9.983154\n147 7.987347  5.8183480 10.156346\n148 7.305348  5.1266252  9.484070\n149 8.482849  6.3094589 10.656239\n150 8.340861  6.1715903 10.510133\n151 8.974637  6.8029505 11.146323\n152 8.349672  6.1814544 10.517889\n153 7.601271  5.4262402  9.776301\n154 4.457111  2.2702778  6.643944\n155 5.011991  2.8385574  7.185425\n156 6.846349  4.6786648  9.014034\n157 7.912964  5.7386066 10.087321\n158 6.562691  4.3832564  8.742126\n159 8.552852  6.3816487 10.724055\n160 7.619833  5.4497828  9.789884\n161 7.518851  5.3487476  9.688954\n162 7.491356  5.3204795  9.662232\n163 7.103079  4.9333382  9.272821\n164 7.049750  4.8838998  9.215601\n165 7.352806  5.1826737  9.522938\n166 7.247764  5.0819322  9.413596\n167 7.272388  5.1057157  9.439060\n168 5.782700  3.6139535  7.951447\n169 7.410985  5.2372431  9.584728\n170 7.788144  5.6196355  9.956653\n171 7.295767  5.1279419  9.463593\n172 7.527060  5.3415467  9.712574\n173 9.062128  6.8900880 11.234167\n174 6.578398  4.4073271  8.749468\n175 6.969795  4.8037395  9.135851\n176 8.014959  5.8475384 10.182379\n177 7.068404  4.9022349  9.234574\n178 7.200625  5.0309046  9.370345\n179 6.960985  4.7914129  9.130557\n180 2.575002  0.3759388  4.774065\n181 5.981847  3.8134030  8.150291\n182 7.358050  5.1826833  9.533417\n183 8.028295  5.8613899 10.195200\n184 7.170602  5.0020106  9.339193\n185 6.942352  4.7766991  9.108005\n186 6.913695  4.7427661  9.084623\n187 7.325580  5.1600216  9.491138\n188 6.953786  4.7872797  9.120293\n189 9.187355  7.0126241 11.362085\n190 6.520969  4.3531578  8.688780\n191 6.432035  4.2616098  8.602460\n192 7.222495  5.0475784  9.397413\n193 7.413329  5.2405050  9.586153\n194 7.144888  4.9779175  9.311859\n195 5.901870  3.7309717  8.072768\n196 1.510635 -0.7063667  3.727637\n197 5.516633  3.3471457  7.686120\n198 7.337161  5.1695173  9.504804\n199 7.338666  5.1724610  9.504871\n200 8.520274  6.3515402 10.689008\n201 6.836956  4.6688607  9.005051\n202 7.905900  5.7333633 10.078438\n203 7.395190  5.2203660  9.570013\n204 6.936931  4.7705001  9.103362\n205 7.021949  4.8552146  9.188684\n206 8.168934  5.9928829 10.344986\n207 7.124095  4.9569318  9.291258\n208 7.887856  5.7200594 10.055653\n209 6.968182  4.7996939  9.136671\n210 6.801321  4.6342304  8.968411\n211 7.036622  4.8700843  9.203159\n212 6.707389  4.5412102  8.873568\n213 7.128750  4.9621118  9.295389\n214 6.686850  4.5207295  8.852971\n215 7.852325  5.6846917 10.019958\n216 7.930119  5.7576930 10.102545\n217 8.049838  5.8679171 10.231759\n218 7.686626  5.5145126  9.858740\n219 6.359693  4.1920272  8.527359\n220 6.364713  4.1932380  8.536188\n221 7.321387  5.1535713  9.489203\n222 7.591973  5.4193323  9.764613\n223 7.958059  5.7885764 10.127541\n224 7.013013  4.8453925  9.180634\n225 8.654337  6.4787392 10.829935\n226 6.706395  4.5397947  8.872996\n227 7.710186  5.5304429  9.889930\n228 7.554958  5.3832014  9.726714\n229 6.894553  4.7242790  9.064827\n230 7.161805  4.9951145  9.328495\n231 7.493389  5.3249331  9.661845\n232 7.284667  5.1145933  9.454740\n233 5.242222  3.0699098  7.414535\n234 3.180773  0.9782909  5.383256\n235 5.547769  3.3659421  7.729596\n236 8.623435  6.4358269 10.811043\n237 7.199718  5.0253491  9.374086\n238 7.006131  4.8379779  9.174284\n239 6.965326  4.7968422  9.133809\n240 6.980275  4.8124994  9.148050\n241 6.499306  4.3251160  8.673497\n242 6.796639  4.6290195  8.964259\n243 7.371286  5.1661050  9.576467\n244 7.431327  5.2610450  9.601609\n245 7.169776  4.9957227  9.343830\n246 7.169301  5.0013924  9.337210\n247 8.147073  5.9616056 10.332541\n248 6.403166  4.2346076  8.571724\n249 6.311015  4.1411746  8.480856\n250 8.176452  6.0077760 10.345128\n251 7.219770  5.0343003  9.405239\n252 7.476066  5.3054231  9.646709\n253 7.167192  4.9856637  9.348719\n254 7.928256  5.7593397 10.097172\n255 7.462280  5.2951897  9.629371\n256 6.767757  4.6002644  8.935250\n257 7.021819  4.8527204  9.190918\n258 8.163833  5.9951334 10.332532\n259 6.706689  4.5379389  8.875439\n260 7.485461  5.2968459  9.674077\n261 7.235941  5.0596285  9.412253\n262 7.906234  5.7374551 10.075013\n263 8.131289  5.9618396 10.300739\n264 6.347607  4.1757278  8.519485\n265 7.741456  5.5708725  9.912039\n266 7.350746  5.1787667  9.522725\n267 7.836845  5.6678365 10.005853\n268 7.712357  5.5385361  9.886177\n269 7.472422  5.3046836  9.640161\n270 6.857357  4.6831571  9.031557\n271 7.195949  5.0201508  9.371746\n272 5.911672  3.7391921  8.084151\n273 6.551110  4.3804709  8.721750\n274 6.575462  4.3995692  8.751355\n275 3.642185  1.4524115  5.831958\n276 7.391555  5.2211521  9.561958\n277 7.003672  4.8332853  9.174058\n278 7.432312  5.2557101  9.608915\n279 7.430921  5.2619049  9.599937\n280 6.982839  4.8095491  9.156128\n281 4.701394  2.5069119  6.895876\n282 7.571214  5.3873164  9.755113\n283 7.530296  5.3563976  9.704194\n284 7.234493  5.0628641  9.406122\n285 7.417701  5.2397729  9.595630\n286 7.841886  5.6721650 10.011606\n287 6.957809  4.7730760  9.142543\n288 7.598812  5.4280153  9.769609\n289 8.184160  6.0118064 10.356514\n290 8.353811  6.1810867 10.526535\n291 0.816149 -1.4279016  3.060200\n292 4.894311  2.7110550  7.077566\n293 7.875087  5.7035277 10.046647\n294 7.426209  5.2512246  9.601194\n295 7.812333  5.6337653  9.990901\n296 8.255838  6.0808237 10.430852\n297 6.299358  4.1242063  8.474509\n298 6.340733  4.1625222  8.518944\n299 7.121022  4.9449893  9.297055\n300 7.494825  5.3180468  9.671603\n301 5.062497  2.8597273  7.265267\n\npredict.lm(linearmodel, newdata = test_data, interval = \"none\")\n\n       1        2        3        4        5        6        7        8 \n7.206459 8.142845 7.689404 6.396338 7.521283 6.791298 6.863208 7.259949 \n       9       10       11       12       13       14       15       16 \n7.604745 6.969011 7.752944 7.057828 6.941123 6.332564 7.767024 6.929965 \n      17       18       19       20       21       22       23       24 \n8.837429 6.353081 7.449417 6.799806 6.972673 7.589810 7.746632 7.848207 \n      25       26       27       28       29       30       31       32 \n5.294945 5.955056 6.092983 7.129457 7.402918 6.292921 6.746848 7.793767 \n      33       34       35       36       37       38       39       40 \n8.256526 6.905086 7.179371 6.433591 7.917666 8.278453 6.828455 7.021316 \n      41       42       43       44       45       46       47       48 \n7.202876 7.940847 7.408036 7.824577 7.609976 6.495756 4.826169 5.177585 \n      49       50       51       52       53       54       55       56 \n5.056527 6.207052 6.928716 6.939441 6.958956 7.582710 7.930428 8.064591 \n      57       58       59       60       61       62       63       64 \n7.050143 7.327507 7.984173 6.564225 7.440815 5.502110 6.780215 7.041163 \n      65       66       67       68       69       70       71       72 \n7.327632 7.501869 6.574142 7.224345 6.590618 7.518297 7.294746 7.902848 \n      73       74       75       76       77       78       79       80 \n7.120466 7.126307 7.539432 7.333818 4.144065 6.818768 8.187750 7.904615 \n      81       82       83       84       85       86       87       88 \n6.891745 7.081912 7.309021 6.695119 8.450645 6.966621 6.480971 7.371525 \n      89       90       91       92       93       94       95       96 \n6.541723 6.384809 6.211872 5.006255 7.593739 7.089950 7.289222 6.647254 \n      97       98       99      100      101      102      103      104 \n8.233174 7.934431 7.012843 9.030049 7.247574 7.232010 7.436129 6.863732 \n     105      106      107      108      109      110      111      112 \n7.196228 7.758461 6.131373 5.661927 6.601054 6.906781 7.728074 7.773756 \n     113      114      115      116      117      118      119      120 \n6.816780 7.072575 6.647445 7.621225 7.223053 7.264403 7.151267 7.473644 \n     121      122      123      124      125      126      127      128 \n6.908264 7.532909 7.297082 8.311321 8.034693 7.236584 6.808971 7.377754 \n     129      130      131      132      133      134      135      136 \n7.515271 6.818418 7.497400 7.802950 7.516205 5.903862 5.695574 6.950652 \n     137      138      139      140      141      142      143      144 \n7.009943 7.304530 8.392232 7.031666 7.977330 7.687166 8.213341 7.574134 \n     145      146      147      148      149      150      151      152 \n7.658685 7.813840 7.987347 7.305348 8.482849 8.340861 8.974637 8.349672 \n     153      154      155      156      157      158      159      160 \n7.601271 4.457111 5.011991 6.846349 7.912964 6.562691 8.552852 7.619833 \n     161      162      163      164      165      166      167      168 \n7.518851 7.491356 7.103079 7.049750 7.352806 7.247764 7.272388 5.782700 \n     169      170      171      172      173      174      175      176 \n7.410985 7.788144 7.295767 7.527060 9.062128 6.578398 6.969795 8.014959 \n     177      178      179      180      181      182      183      184 \n7.068404 7.200625 6.960985 2.575002 5.981847 7.358050 8.028295 7.170602 \n     185      186      187      188      189      190      191      192 \n6.942352 6.913695 7.325580 6.953786 9.187355 6.520969 6.432035 7.222495 \n     193      194      195      196      197      198      199      200 \n7.413329 7.144888 5.901870 1.510635 5.516633 7.337161 7.338666 8.520274 \n     201      202      203      204      205      206      207      208 \n6.836956 7.905900 7.395190 6.936931 7.021949 8.168934 7.124095 7.887856 \n     209      210      211      212      213      214      215      216 \n6.968182 6.801321 7.036622 6.707389 7.128750 6.686850 7.852325 7.930119 \n     217      218      219      220      221      222      223      224 \n8.049838 7.686626 6.359693 6.364713 7.321387 7.591973 7.958059 7.013013 \n     225      226      227      228      229      230      231      232 \n8.654337 6.706395 7.710186 7.554958 6.894553 7.161805 7.493389 7.284667 \n     233      234      235      236      237      238      239      240 \n5.242222 3.180773 5.547769 8.623435 7.199718 7.006131 6.965326 6.980275 \n     241      242      243      244      245      246      247      248 \n6.499306 6.796639 7.371286 7.431327 7.169776 7.169301 8.147073 6.403166 \n     249      250      251      252      253      254      255      256 \n6.311015 8.176452 7.219770 7.476066 7.167192 7.928256 7.462280 6.767757 \n     257      258      259      260      261      262      263      264 \n7.021819 8.163833 6.706689 7.485461 7.235941 7.906234 8.131289 6.347607 \n     265      266      267      268      269      270      271      272 \n7.741456 7.350746 7.836845 7.712357 7.472422 6.857357 7.195949 5.911672 \n     273      274      275      276      277      278      279      280 \n6.551110 6.575462 3.642185 7.391555 7.003672 7.432312 7.430921 6.982839 \n     281      282      283      284      285      286      287      288 \n4.701394 7.571214 7.530296 7.234493 7.417701 7.841886 6.957809 7.598812 \n     289      290      291      292      293      294      295      296 \n8.184160 8.353811 0.816149 4.894311 7.875087 7.426209 7.812333 8.255838 \n     297      298      299      300      301 \n6.299358 6.340733 7.121022 7.494825 5.062497 \n\nlibrary(ISLR2)\n\n\nAttaching package: 'ISLR2'\n\nThe following objects are masked from 'package:ISLR':\n\n    Auto, Credit\n\nlibrary(CombMSC)\n\n\nAttaching package: 'CombMSC'\n\nThe following object is masked from 'package:stats':\n\n    BIC\n\nlibrary(MASS)\n\n\nAttaching package: 'MASS'\n\nThe following object is masked from 'package:ISLR2':\n\n    Boston\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\nlibrary(glmnet)\n\nLoading required package: Matrix\n\nAttaching package: 'Matrix'\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\nLoaded glmnet 4.1-8\n\nlibrary(foreign)\nlibrary(boot)\n\n\nAttaching package: 'boot'\n\nThe following object is masked from 'package:psych':\n\n    logit\n\nThe following object is masked from 'package:lattice':\n\n    melanoma\n\nlibrary(MASS)\nlibrary(leaps)\n\n\nmodel_full&lt;-lm(weight ~ fage+mage+weeks+visits+gained, data = train_data)\ny.train = train_data$weight\npredictors.train = train_data[,-6]\n\ny.test = test_data$weight\npredictors.test = test_data[,-6]\n\nRidgeCV = cv.glmnet(as.matrix(predictors.train), y.train,\n                     family='gaussian', type.measure=\"mse\", alpha=0, nfolds=10)\n\nplot(RidgeCV)\n\n\n\nRidgeCV$lambda\n\n  [1] 1019.4933751  928.9243935  846.4013106  771.2093509  702.6972377\n  [6]  640.2715518  583.3915918  531.5646907  484.3419486  441.3143448\n [11]  402.1091947  366.3869221  333.8381178  304.1808595  277.1582703\n [16]  252.5362934  230.1016652  209.6600674  191.0344449  174.0634713\n [21]  158.6001522  144.5105518  131.6726327  119.9751989  109.3169329\n [26]   99.6055179   90.7568382   82.6942508   75.3479215   68.6542198\n [31]   62.5551682   56.9979394   51.9343996   47.3206907   43.1168509\n [36]   39.2864687   35.7963671   32.6163165   29.7187729   27.0786390\n [41]   24.6730473   22.4811618   20.4839974   18.6642555   17.0061745\n [46]   15.4953928   14.1188248   12.8645473   11.7216964   10.6803732\n [51]    9.7315583    8.8670335    8.0793106    7.3615670    6.7075856\n [56]    6.1117022    5.5687554    5.0740426    4.6232786    4.2125593\n [61]    3.8383271    3.4973407    3.1866466    2.9035537    2.6456100\n [66]    2.4105813    2.1964319    2.0013069    1.8235163    1.6615202\n [71]    1.5139153    1.3794232    1.2568791    1.1452214    1.0434831\n [76]    0.9507829    0.8663180    0.7893567    0.7192324    0.6553378\n [81]    0.5971194    0.5440730    0.4957390    0.4516989    0.4115712\n [86]    0.3750084    0.3416937    0.3113385    0.2836801    0.2584787\n [91]    0.2355162    0.2145936    0.1955297    0.1781594    0.1623322\n [96]    0.1479110    0.1347710    0.1227983    0.1118893    0.1019493\n\nRidgeCV$cvm\n\n  [1] 2.287161 2.284553 2.283307 2.282899 2.282451 2.281960 2.281422 2.280831\n  [9] 2.280184 2.279474 2.278696 2.277843 2.276908 2.275884 2.274761 2.273531\n [17] 2.272184 2.270708 2.269093 2.267324 2.265389 2.263271 2.260954 2.258421\n [25] 2.255652 2.252627 2.249322 2.245715 2.241778 2.237485 2.232806 2.227710\n [33] 2.222164 2.216131 2.209576 2.202459 2.194741 2.186379 2.177331 2.167553\n [41] 2.157000 2.145630 2.133397 2.120260 2.106180 2.091118 2.075044 2.057929\n [49] 2.039751 2.020499 2.000167 1.978762 1.956302 1.932817 1.908351 1.882963\n [57] 1.856728 1.829735 1.802088 1.773906 1.745321 1.716476 1.687524 1.658624\n [65] 1.629939 1.601632 1.573863 1.546786 1.520544 1.495269 1.471076 1.448062\n [73] 1.426307 1.405869 1.386786 1.369077 1.352742 1.337762 1.324104 1.311721\n [81] 1.300556 1.290542 1.281608 1.273675 1.266665 1.260500 1.255100 1.250392\n [89] 1.246302 1.242763 1.239712 1.237091 1.234848 1.232933 1.231305 1.229923\n [97] 1.228755 1.227770 1.226941 1.226102\n\nmin(RidgeCV$cvm) \n\n[1] 1.226102\n\nRidgeCV$lambda.min\n\n[1] 0.1019493\n\nRidgePredictTest&lt;-predict(RidgeCV,s=RidgeCV$lambda.min,newx=as.matrix(predictors.test))\nMSPE_Ridge_test&lt;-mean((RidgePredictTest-y.test)^2)\nMSPE_linear&lt;-MSPE_Ridge_test\nprint(MSPE_linear)\n\n[1] 1.243111\n\nlibrary(stats)\nlibrary(mgcv)\n\nlibrary(ISLR2)\n\nlibrary(splines)\n\nlinearmodel&lt;- lm(weight ~ fage+mage+weeks+visits+gained, data = train_data)\nsummary(linearmodel)\n\n\nCall:\nlm(formula = weight ~ fage + mage + weeks + visits + gained, \n    data = train_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.5095 -0.7245 -0.0344  0.7819  4.3711 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -7.069515   0.581129 -12.165  &lt; 2e-16 ***\nfage         0.014971   0.009724   1.540  0.12413    \nmage         0.010142   0.009666   1.049  0.29445    \nweeks        0.339776   0.014522  23.397  &lt; 2e-16 ***\nvisits       0.012027   0.010808   1.113  0.26622    \ngained       0.008512   0.003048   2.793  0.00537 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.102 on 692 degrees of freedom\nMultiple R-squared:  0.4722,    Adjusted R-squared:  0.4683 \nF-statistic: 123.8 on 5 and 692 DF,  p-value: &lt; 2.2e-16\n\n#test on single variable (weeks)\nmodel_high = gam(weight ~ s(weeks, bs = \"cr\", sp=300), data = train_data)\nmodel_high\n\n\nFamily: gaussian \nLink function: identity \n\nFormula:\nweight ~ s(weeks, bs = \"cr\", sp = 300)\n\nEstimated degrees of freedom:\n3.96  total = 4.96 \n\nGCV score: 1.13054     \n\nmodel_low = gam(weight ~ s(weeks, bs = \"cr\", sp=.00000000000001), data = train_data)\nmodel_OK = gam(weight ~ s(weeks, bs = \"cr\"), data = train_data, method = \"REML\")\nmodel_OK$sp\n\ns(weeks) \n 118.113 \n\nplot(model_high, shade = TRUE)\n\n\n\nplot(model_low, shade = TRUE)\n\n\n\nplot(model_OK, shade = TRUE)\n\n\n\nmodel_lm2 = gam(weight ~ s(fage)+s(mage)+s(weeks)+s(visits)+s(gained), data = train_data, method=\"REML\")\nmodel_lm2\n\n\nFamily: gaussian \nLink function: identity \n\nFormula:\nweight ~ s(fage) + s(mage) + s(weeks) + s(visits) + s(gained)\n\nEstimated degrees of freedom:\n1.00 1.00 4.84 4.27 1.00  total = 13.11 \n\nREML score: 1033.711     \n\n#removing 3 variables with 1 estimated degrees of freedom- fage, mage, gained for linear modeling to be included as Parametric Coefficients\ngam_model = gam(weight ~ fage+ mage +s(weeks)+s(visits)+ gained, data = train_data, method=\"REML\")\n# we keep seeks and visits within the smoothing function\nsummary(gam_model)\n\n\nFamily: gaussian \nLink function: identity \n\nFormula:\nweight ~ fage + mage + s(weeks) + s(visits) + gained\n\nParametric coefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 6.239524   0.224609  27.780   &lt;2e-16 ***\nfage        0.017583   0.009178   1.916   0.0558 .  \nmage        0.004097   0.009170   0.447   0.6552    \ngained      0.006518   0.002891   2.254   0.0245 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nApproximate significance of smooth terms:\n            edf Ref.df       F p-value    \ns(weeks)  4.840  5.882 108.171  &lt;2e-16 ***\ns(visits) 4.266  5.260   3.147  0.0072 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-sq.(adj) =   0.53   Deviance explained = 53.8%\n-REML =   1040  Scale est. = 1.0728    n = 698\n\nprint(gam_model)\n\n\nFamily: gaussian \nLink function: identity \n\nFormula:\nweight ~ fage + mage + s(weeks) + s(visits) + gained\n\nEstimated degrees of freedom:\n4.84 4.27  total = 13.11 \n\nREML score: 1039.988     \n\n#predict() function to predict the weight variable in the test_data dataset using gam_model\npredicted_weights_gam&lt;-predict(gam_model, test_data)\npredicted_weights_gam\n\n        1         2         3         4         5         6         7         8 \n7.5440143 7.6155990 7.3184928 6.7480857 7.2873284 7.2346419 7.3264115 7.4400624 \n        9        10        11        12        13        14        15        16 \n7.4191122 7.2572187 7.6323616 7.3330885 6.8151949 6.4087828 7.5764664 7.1392739 \n       17        18        19        20        21        22        23        24 \n7.6417958 6.6162236 7.4267417 7.0481855 6.9665285 7.3591771 7.4691362 7.7016345 \n       25        26        27        28        29        30        31        32 \n4.7716174 6.2374892 6.4370486 7.1600173 7.4816381 6.6762571 7.0920163 7.5957261 \n       33        34        35        36        37        38        39        40 \n7.6500289 7.2368716 7.2778003 6.4421988 7.8677313 7.5156636 7.2150568 7.4364665 \n       41        42        43        44        45        46        47        48 \n7.5463307 7.7936875 7.5731755 7.4599415 7.7094360 6.8866470 4.0640961 5.1488183 \n       49        50        51        52        53        54        55        56 \n4.7521994 6.4896856 7.3129328 7.2436766 7.3728091 7.5398262 7.7756135 7.8475995 \n       57        58        59        60        61        62        63        64 \n7.2142188 7.5645070 7.8227471 6.7429963 7.6941072 5.4857331 7.1215962 7.3240152 \n       65        66        67        68        69        70        71        72 \n7.4491908 7.5171013 6.9948451 7.5434989 6.7571388 7.4807169 7.3037202 7.8869041 \n       73        74        75        76        77        78        79        80 \n7.3133586 7.4561916 7.4439332 7.6392534 3.5263701 6.9291990 7.6805314 7.6060285 \n       81        82        83        84        85        86        87        88 \n7.1283890 6.9274786 7.5438017 7.0712182 7.7471023 7.3835817 6.8214785 7.4546484 \n       89        90        91        92        93        94        95        96 \n6.9663542 6.6580606 6.0368913 4.4262349 7.2402510 7.3838488 7.5127323 7.0128868 \n       97        98        99       100       101       102       103       104 \n7.5711347 7.6680323 7.2419245 7.4763764 7.5167792 7.5542714 7.7520003 7.1155118 \n      105       106       107       108       109       110       111       112 \n7.3762719 7.8796125 6.4848220 5.4646193 6.9922171 7.3181983 7.6792970 7.6596343 \n      113       114       115       116       117       118       119       120 \n7.1570867 7.0822144 6.8098121 7.5607624 7.4828808 7.1111190 7.5977473 7.4705533 \n      121       122       123       124       125       126       127       128 \n7.2131297 7.6726537 7.3238965 7.3827486 7.8291797 7.5169371 6.7110227 7.6866651 \n      129       130       131       132       133       134       135       136 \n7.5163572 7.2321660 7.5960558 7.9344897 7.5031413 5.8996759 5.3323819 7.2566554 \n      137       138       139       140       141       142       143       144 \n7.2507276 7.5963284 7.7625272 7.3309274 7.8299045 7.8115650 7.9553918 7.5393980 \n      145       146       147       148       149       150       151       152 \n7.7588611 7.6988978 7.6689191 7.1095694 7.6055998 7.6987119 7.8055521 7.7299393 \n      153       154       155       156       157       158       159       160 \n7.7626862 3.3895481 4.7284018 7.2768302 7.6572598 6.6891232 7.8045656 7.7869562 \n      161       162       163       164       165       166       167       168 \n7.7035024 7.5338437 7.2817030 7.4325578 7.4501232 7.5736891 7.4252708 5.8807086 \n      169       170       171       172       173       174       175       176 \n7.5741059 7.7020448 7.6434854 7.8025741 7.7649163 6.6301564 7.3060323 7.8390000 \n      177       178       179       180       181       182       183       184 \n7.4431724 7.2402436 7.2748049 2.0849653 6.0364294 7.4301295 7.7973661 7.4395004 \n      185       186       187       188       189       190       191       192 \n7.3429879 7.0986167 7.6320544 7.2786515 8.0366322 6.9036750 6.8358747 7.1289917 \n      193       194       195       196       197       198       199       200 \n7.6751013 7.5286680 5.8834761 1.3743371 5.4119876 7.4883420 7.6012376 7.8642530 \n      201       202       203       204       205       206       207       208 \n7.0936368 7.9524843 7.5239341 7.3165124 7.3716821 7.9585644 7.4498932 7.8921570 \n      209       210       211       212       213       214       215       216 \n7.0801190 7.1477203 7.3645555 7.0513170 7.3233535 7.0603672 7.6749737 7.9000351 \n      217       218       219       220       221       222       223       224 \n7.6316057 7.8586614 6.6217218 6.4321333 7.5177545 7.7396757 7.8179138 7.3672920 \n      225       226       227       228       229       230       231       232 \n7.8394464 7.0716765 7.8434695 7.3385135 7.1846978 7.5058385 7.7429710 7.5713483 \n      233       234       235       236       237       238       239       240 \n4.8626006 2.5690702 5.1472270 7.7633316 7.5577195 7.3583538 7.2965080 7.3509348 \n      241       242       243       244       245       246       247       248 \n6.8302194 6.9820744 7.8121370 7.7040234 7.5225530 7.4657150 8.1316122 6.6657372 \n      249       250       251       252       253       254       255       256 \n6.6077276 7.8288977 7.3146296 7.6167472 6.9156129 7.8301568 7.7241932 7.0987746 \n      257       258       259       260       261       262       263       264 \n7.3368817 7.8314763 6.9034123 7.6871540 7.2452730 7.7113656 7.9183015 6.4001030 \n      265       266       267       268       269       270       271       272 \n7.7898679 7.6887591 7.8323035 7.3458036 7.7282903 7.0808267 7.4919799 6.0362292 \n      273       274       275       276       277       278       279       280 \n6.5272656 6.3875492 2.6880641 7.5018586 7.3703922 7.1498256 7.7115137 7.3911629 \n      281       282       283       284       285       286       287       288 \n3.9323484 7.8624715 7.8451254 7.1220160 7.7568753 7.9386399 6.9169760 7.5880071 \n      289       290       291       292       293       294       295       296 \n7.9448959 7.9989870 0.5032178 4.3780738 7.9689652 7.3811901 7.4787576 8.0839058 \n      297       298       299       300       301 \n6.3759338 6.3467639 7.4969858 7.7118394 4.3020275 \n\nMSPE_gam&lt;-mean((predicted_weights_gam-y.test)^2)\n\nprint(MSPE_gam)\n\n[1] 1.159199\n\n# Please provide your code for Task 5 in this code chunk\n\n\n\n\n#linear model\nlinearmodel\n\n\nCall:\nlm(formula = weight ~ fage + mage + weeks + visits + gained, \n    data = train_data)\n\nCoefficients:\n(Intercept)         fage         mage        weeks       visits       gained  \n  -7.069515     0.014971     0.010142     0.339776     0.012027     0.008512  \n\nMSPE_linear\n\n[1] 1.243111\n\nplot(linearmodel)\n\n\n\n\n\n\n\n\n\n\n\n\ngam_model\n\n\nFamily: gaussian \nLink function: identity \n\nFormula:\nweight ~ fage + mage + s(weeks) + s(visits) + gained\n\nEstimated degrees of freedom:\n4.84 4.27  total = 13.11 \n\nREML score: 1039.988     \n\nMSPE_gam\n\n[1] 1.159199\n\nplot(gam_model)\n\n\n\n\n\n\n#both the gam model and linear model are very close - 0.083 difference, but the MSPE gam does have the lower Mean Squared prediction error (MSPE), and therefore is the more superior of the models to use in this situation for testing."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Jonathan Jelliff",
    "section": "",
    "text": "This blog’s purpose is to share Data Analytics and Data Science projects and to showcase prior analysis and techniques."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Analysis Home Page",
    "section": "",
    "text": "Jonathan Jelliff’s Resume\n\n\n\n\n\n\n\nResume\n\n\n\n\n\n\n\n\n\n\n\nJonathan Jelliff\n\n\n\n\n\n\n  \n\n\n\n\nPost With Code\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nDec 6, 2023\n\n\nJonathan Jelliff\n\n\n\n\n\n\n  \n\n\n\n\nWelcome to my Data Analytics Platform\n\n\n\n\n\n\n\nR Code\n\n\nData Analytics\n\n\nData Science\n\n\n\n\n\n\n\n\n\n\n\nDec 3, 2023\n\n\nJonathan Jelliff\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Resume/index.html",
    "href": "posts/Resume/index.html",
    "title": "Jonathan Jelliff’s Resume",
    "section": "",
    "text": "Jonathan Jelliff's Resume\n\n\nResume Link\nPlease check out the attacheched hyperlink to advance to my Resume.\nWelcome, I would like to introduce myself as an Analyst whom is passionate about all thing Mathematical deep learning into problem solving. I like admire W. Edwards Deming’s famous quote that “Without data, you’re just another person with an opinion.” I believe that as Data Analyst and Data Scientist we have the privilege to go beyond just a mere opinion, in that we have the tools necessary to prove facts. This, in turn, allows us to hopefully use these skills to better the life of our fellow man, as data can be seen in everything and everywhere- whether it is realized or not.\nAs a U.S. Navy Veteran, I first knew I was inclined to Mathematics and Mechanical structures when I took the ASVAB (Military’s I.Q. entrance exam), and I scored the highest on mathematics. the recruiter came back and listed numerous jobs in which I was eligible, and I chose Aviation Structural Mechanics. This involved numerous calculations and the required knowledge of formulas and chemical reactions to calculate load and stress factors, in determining outside variables such as temperature and age had on certain components. After the Navy, I began teaching Mathematics at a High School level, which I enjoyed greatly. During COVID though, as schools were closed, I had some time to research how to pursue my career, and found out that Data Analyst, Data Scientist, and Mathematicians were among the fastest growing careers. So chose to combine my passion for Mathematics with technology to lead me to where I am today. Enjoying greatly the deep challenge of problem solving at a complex level while still maintaing the need to work in a team."
  }
]